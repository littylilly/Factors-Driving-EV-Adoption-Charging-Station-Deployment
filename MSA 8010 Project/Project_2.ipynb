{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project 2**\n",
        "\n",
        "**Deadline:** December 01, 2024, 11:59pm ET.\n",
        "\n",
        "\n",
        "\n",
        "**Group Members:**\n",
        "\n",
        "Dorothy (Gracie) Rehberg - drehberg1@student.gsu.edu\n",
        "\n",
        "Lilly Parham - lparham2@student.gsu.edu\n",
        "\n",
        "Pamela Alvarado-Zarate - palvaradozarate1@student.gsu.edu\n",
        "\n",
        "\n",
        "\n",
        "**Team Name:** Eco-Warriors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P6Eyd9TnCDfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comments / Brainstorming 11/13**\n",
        "### **EV Registration Data by County/Zip**: https://www.atlasevhub.com/materials/state-ev-registration-data/"
      ],
      "metadata": {
        "id": "51iIDBDSNYTP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaKOGJoYTzho"
      },
      "source": [
        "# Import Libraries:\n",
        "\n",
        "When working with data analysis, downloading files, and geospatial data, it's essential to have the right tools at our disposal. Each of the libraries we've imported serves a specific purpose, making it easier to handle various aspects of our project.\n",
        "\n",
        "The following libraries will allow us to:\n",
        "- Efficiently download and extract data from various sources.\n",
        "- Manipulate and analyze structured datasets using pandas.\n",
        "- Visualize data to extract insights using matplotlib and seaborn.\n",
        "- Handle geospatial data with geopandas for mapping and spatial analysis.\n",
        "- Perform geocoding and template-based string manipulations to automate repetitive tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq7ytTC-S_Th"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries for data retrieval, extraction, and analysis:\n",
        "\n",
        "# Libraries for downloading and handling files:\n",
        "import os # os: for joining filenames and filepaths\n",
        "import io # io: for handling input and output operations, particularly in-memory file handling\n",
        "import zipfile # zipfile: for extracting compressed files\n",
        "\n",
        "# Libraries for making HTTP requests and retrieving data:\n",
        "import requests # requests: for sending HTTP requests to access online data\n",
        "\n",
        "# Data manipulation, analysis, and visualization:\n",
        "import numpy as np # numpy: for matrix operations\n",
        "import pandas as pd # pandas: for data manipulation and analysis, especially for working with structured data\n",
        "import matplotlib.pyplot as plt # matplotlib.pyplot: for creating plots, charts, and visualizations\n",
        "import seaborn as sns # seaborn: for statistical data visualization, enhancing matplotlib's capabilities\n",
        "\n",
        "# Geographic libraries:\n",
        "from shapely.geometry import Point\n",
        "import geopandas as gpd # geopandas: for reading shapefiles and performing geometric calculations\n",
        "\n",
        "# libraries for candidate models:\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# libraries for model tuning:\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "from string import Template\n",
        "google_drive_file_download_url = Template(\"https://drive.usercontent.google.com/uc?id=$file_id&authuser=0&export=download\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ennXOasaT88u"
      },
      "source": [
        "# Load datasets:\n",
        "Loading the datasets into the code is a critical first step in any data analysis, exploration, and machine learning project. This will allow us to explore, clean, and transform your data, which in turn sets the stage for building and evaluating machine learning models.\n",
        "\n",
        "Below we have linked and imported all the neccesary files needed to address our business problem and determine a business solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znT5tzFxUDBV",
        "outputId": "7a0e0ba9-2b2a-4f24-cced-385f9abe998e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  STATE STUSAB  STATE_NAME  STATENS\n",
            "0    01     AL     Alabama  1779775\n",
            "1    02     AK      Alaska  1785533\n",
            "2    04     AZ     Arizona  1779777\n",
            "3    05     AR    Arkansas    68085\n",
            "4    06     CA  California  1779778\n"
          ]
        }
      ],
      "source": [
        "# Function to download and load state abbreviations data from the US Census.\n",
        "def load_postal_codes():\n",
        "    \"\"\"\n",
        "    State abbreviations reference from the US Census\n",
        "    https://www.census.gov/library/reference/code-lists/ansi/ansi-codes-for-states.html\n",
        "\n",
        "    Filename:\n",
        "    state.txt\n",
        "\n",
        "    URL for direct download:\n",
        "    https://www2.census.gov/geo/docs/reference/state.txt\n",
        "    \"\"\"\n",
        "    # Google Drive file ID for accessing the state abbreviations data\n",
        "    file_id = \"1L0_KbZAdouKTDt5wX1A_g1CxHWQ1dToj\"\n",
        "    # Construct the file download URL using the provided Google Drive file ID\n",
        "    file_url = google_drive_file_download_url.substitute(file_id=file_id)\n",
        "    # download the data from url and read into a pandas DataFrame\n",
        "    return pd.read_csv(\n",
        "        file_url,\n",
        "        delimiter=\"|\", # this file is pipe-separated, rather than comma-separated\n",
        "        dtype = {'STATE': 'string'}, # need to make sure the FIPS code gets read as a string so it doesn't cut off leading zero\n",
        "    )\n",
        "\n",
        "def load_FIPS_codes():\n",
        "    \"\"\"\n",
        "    FIPS code reference from the US Census for county and county equivalents\n",
        "    https://www.census.gov/library/reference/code-lists/ansi.html#cou\n",
        "\n",
        "    Filename:\n",
        "    national_county2020.txt\n",
        "\n",
        "    URL for direct download:\n",
        "    https://www2.census.gov/geo/docs/reference/codes2020/national_county2020.txt\n",
        "    \"\"\"\n",
        "    file_id = \"15OPIhf823UYcrlWipZ-uUxNYxdH9bW4o\"\n",
        "    file_url = google_drive_file_download_url.substitute(file_id=file_id)\n",
        "    df = pd.read_csv(\n",
        "        file_url,\n",
        "        delimiter=\"|\", # this file is pipe-separated, rather than comma-separated\n",
        "        dtype = { # need to make sure the FIPS code gets read as a string so it doesn't cut off leading zero\n",
        "            'STATE': 'string',\n",
        "            'STATEFP': 'string',\n",
        "            'COUNTYFP': 'string',\n",
        "            'COUNTYNS': 'string',\n",
        "            'CLASSFP': 'string',\n",
        "            'FUNCSTAT': 'string',\n",
        "        } )\n",
        "    df['FIPS'] = df['STATEFP'] + df['COUNTYFP']\n",
        "    return df\n",
        "\n",
        "# Load the state postal codes into a DataFrame\n",
        "state_df = load_postal_codes()\n",
        "# Create a dictionary mapping state names to state abbreviations\n",
        "states_to_abbreviations = state_df.set_index('STATE_NAME').STUSAB\n",
        "# Create a dictionary mapping FIPS codes to state abbreviations\n",
        "FIPS_to_abbreviations = state_df.set_index('STATE').STUSAB\n",
        "# Display the first few rows of the DataFrame for verification\n",
        "print(state_df.head())\n",
        "\n",
        "FIPS_codes_df = load_FIPS_codes()\n",
        "Name_to_FIPS = FIPS_codes_df.set_index([\"STATE\", \"COUNTYNAME\"]).FIPS\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ZipCode_FIPS_conversion():\n",
        "    \"\"\"\n",
        "    Table to convert from Zip postal codes to County FIPS codes\n",
        "    from the Department of Housing and Urban Development\n",
        "    Cannot be downloaded from a stable url (requires active session with cookies)\n",
        "\n",
        "    https://www.huduser.gov/portal/datasets/usps_crosswalk.html\n",
        "\n",
        "    Filename:\n",
        "    ZIP_COUNTY_092024.xlsx\n",
        "    \"\"\"\n",
        "    file_id = '1BReI2u0Hg_2vwMMpTCOhk5Fh03tORdi_'\n",
        "    file_url = google_drive_file_download_url.substitute(file_id=file_id)\n",
        "    df = pd.read_excel(file_url)\n",
        "    for col in [\"ZIP\", \"COUNTY\"]:\n",
        "        df[col] = df[col].astype(str).str.zfill(5)\n",
        "    return df\n",
        "\n",
        "ZipCode_FIPS_df = load_ZipCode_FIPS_conversion()\n",
        "print(ZipCode_FIPS_df)"
      ],
      "metadata": {
        "id": "1V0AK4WupfH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb033d1-d0fa-452d-d3b8-0560abba0f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         ZIP COUNTY USPS_ZIP_PREF_CITY USPS_ZIP_PREF_STATE  RES_RATIO  \\\n",
            "0      00501  36103         HOLTSVILLE                  NY   0.000000   \n",
            "1      00601  72001           ADJUNTAS                  PR   0.997449   \n",
            "2      00601  72081           ADJUNTAS                  PR   0.002551   \n",
            "3      00602  72003             AGUADA                  PR   0.999413   \n",
            "4      00602  72117             AGUADA                  PR   0.000587   \n",
            "...      ...    ...                ...                 ...        ...   \n",
            "54559  99925  02198            KLAWOCK                  AK   1.000000   \n",
            "54560  99926  02198         METLAKATLA                  AK   1.000000   \n",
            "54561  99927  02198        POINT BAKER                  AK   0.000000   \n",
            "54562  99928  02130          WARD COVE                  AK   0.000000   \n",
            "54563  99929  02275           WRANGELL                  AK   1.000000   \n",
            "\n",
            "       BUS_RATIO  OTH_RATIO  TOT_RATIO  \n",
            "0       1.000000   0.000000   1.000000  \n",
            "1       0.994975   0.987805   0.997051  \n",
            "2       0.005025   0.012195   0.002949  \n",
            "3       0.998987   1.000000   0.999392  \n",
            "4       0.000000   0.000000   0.000532  \n",
            "...          ...        ...        ...  \n",
            "54559   0.000000   1.000000   1.000000  \n",
            "54560   0.000000   1.000000   1.000000  \n",
            "54561   0.000000   1.000000   1.000000  \n",
            "54562   0.000000   1.000000   1.000000  \n",
            "54563   1.000000   1.000000   1.000000  \n",
            "\n",
            "[54564 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uptWf2bWs-eI",
        "outputId": "1cf5546c-60f0-4a60-bbe8-663e8f36efd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     STATE     NAME  SEX  AGE  POPEST2023_CIV State\n",
            "261      1  Alabama    0    0           57885    AL\n",
            "262      1  Alabama    0    1           58419    AL\n",
            "263      1  Alabama    0    2           58006    AL\n",
            "264      1  Alabama    0    3           59016    AL\n",
            "265      1  Alabama    0    4           59779    AL\n"
          ]
        }
      ],
      "source": [
        "# Function to download and load US state population estimates data from the Census Bureau.\n",
        "def load_population_estimates():\n",
        "    \"\"\"\n",
        "    From the Census State Population by Characteristics: 2020-2023\n",
        "    Page: https://www.census.gov/data/datasets/time-series/demo/popest/2020s-state-detail.html\n",
        "    Datasets: https://www2.census.gov/programs-surveys/popest/datasets/2020-2023/state/asrh/\n",
        "    Codebooks: https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2020-2023/\n",
        "    Our specific codebook: https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2020-2023/SC-EST2023-AGESEX-CIV.pdf\n",
        "\n",
        "    Filename:\n",
        "    sc-est2023-agesex-civ.csv\n",
        "\n",
        "    URL for direct download:\n",
        "    https://www2.census.gov/programs-surveys/popest/datasets/2020-2023/state/asrh/sc-est2023-agesex-civ.csv\n",
        "    \"\"\"\n",
        "    # Google Drive file ID to access the specific population estimates dataset\n",
        "    file_id = '1azekJW1FKwhdjXuqhn7bTLhGMJ-qfzgW'\n",
        "    # Generate the download URL using the Google Drive file ID\n",
        "    file_url = google_drive_file_download_url.substitute(file_id=file_id)\n",
        "    # Read the CSV data into a pandas DataFrame\n",
        "    df = pd.read_csv(file_url)\n",
        "    # Select only the relevant columns from the dataset\n",
        "    df = df[[\"STATE\", \"NAME\", \"SEX\", \"AGE\", \"POPEST2023_CIV\"]]\n",
        "    # Remove the row that corresponds to \"United States\" as a whole to focus on state-specific data\n",
        "    df = df[df[\"NAME\"] != \"United States\"]\n",
        "    # Map full state names to their corresponding abbreviations using the 'states_to_abbreviations' dictionary\n",
        "    df[\"State\"] = df[\"NAME\"].apply(states_to_abbreviations.get)\n",
        "    return df\n",
        "\n",
        "# Load the population estimates data and store it in a DataFrame, printing the head of the data for verification\n",
        "population_estimates_df = load_population_estimates()\n",
        "print(population_estimates_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to download and load US state population estimates data from the Census Bureau.\n",
        "def load_population_estimates_county():\n",
        "    \"\"\"\n",
        "    From the Census County Population by Characteristics: 2020-2023\n",
        "    Page: https://www.census.gov/data/tables/time-series/demo/popest/2020s-counties-detail.html\n",
        "    Datasets: https://www2.census.gov/programs-surveys/popest/datasets/2020-2023/counties/asrh/\n",
        "    Codebooks: https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2020-2023/\n",
        "    Our specific codebook: https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2020-2023/CC-EST2023-SYASEX.pdf\n",
        "\n",
        "    The script to download all 52 datasets individually and compile the data into\n",
        "    a single aggregated .csv dataset is saved in our project folder as build_syasex.py\n",
        "    https://drive.google.com/file/d/1D9Jh9shEhlZrEkVL-c4L6o4cKHf2ZU8p/view?usp=sharing\n",
        "\n",
        "    The file created by the script is named: cc-est2023-syasex-all.csv\n",
        "    and is saved in our DATASETS folder\n",
        "    \"\"\"\n",
        "    # Google Drive file ID to access the specific population estimates dataset\n",
        "    file_id = '12DzLf7Wvn-u1qF9Ez4rHxEEcKB9QOVOG'\n",
        "    # Generate the download URL using the Google Drive file ID\n",
        "    file_url = google_drive_file_download_url.substitute(file_id=file_id)\n",
        "    # Read the CSV data into a pandas DataFrame\n",
        "    df = pd.read_csv(file_url, dtype = {\n",
        "        \"SUMLEV\": str,\n",
        "        \"STATE\": str,\n",
        "        \"COUNTY\": str,\n",
        "    })\n",
        "    df = df[df[\"YEAR\"] == 5]\n",
        "    df[\"FIPS\"] = df[\"STATE\"] + df[\"COUNTY\"]\n",
        "    # Select only the relevant columns from the dataset\n",
        "    df[\"State\"] = df[\"STNAME\"].apply(states_to_abbreviations.get)\n",
        "    df = df[[\"FIPS\", \"STATE\", \"State\", \"STNAME\", \"COUNTY\", \"CTYNAME\", \"AGE\", \"TOT_POP\", \"TOT_MALE\", \"TOT_FEMALE\"]]\n",
        "    df = df.reset_index()\n",
        "    # Map full state names to their corresponding abbreviations using the 'states_to_abbreviations' dictionary\n",
        "    return df\n",
        "\n",
        "# Load the population estimates data and store it in a DataFrame, printing the head of the data for verification\n",
        "population_estimates_county_df = load_population_estimates_county()\n",
        "print(population_estimates_county_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzc_dDChnnTr",
        "outputId": "b500a2d7-75b3-4fdc-d7e3-2205130b45e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          index   FIPS STATE State                    STNAME COUNTY  \\\n",
            "0           344  01001    01    AL                   Alabama    001   \n",
            "1           345  01001    01    AL                   Alabama    001   \n",
            "2           346  01001    01    AL                   Alabama    001   \n",
            "3           347  01001    01    AL                   Alabama    001   \n",
            "4           348  01001    01    AL                   Alabama    001   \n",
            "...         ...    ...   ...   ...                       ...    ...   \n",
            "277087  1385455  72153    72  None  Puerto Rico Commonwealth    153   \n",
            "277088  1385456  72153    72  None  Puerto Rico Commonwealth    153   \n",
            "277089  1385457  72153    72  None  Puerto Rico Commonwealth    153   \n",
            "277090  1385458  72153    72  None  Puerto Rico Commonwealth    153   \n",
            "277091  1385459  72153    72  None  Puerto Rico Commonwealth    153   \n",
            "\n",
            "                CTYNAME  AGE  TOT_POP  TOT_MALE  TOT_FEMALE  \n",
            "0        Autauga County    0      705       358         347  \n",
            "1        Autauga County    1      736       391         345  \n",
            "2        Autauga County    2      704       369         335  \n",
            "3        Autauga County    3      715       365         350  \n",
            "4        Autauga County    4      757       408         349  \n",
            "...                 ...  ...      ...       ...         ...  \n",
            "277087  Yauco Municipio   81      246       117         129  \n",
            "277088  Yauco Municipio   82      216       103         113  \n",
            "277089  Yauco Municipio   83      201        79         122  \n",
            "277090  Yauco Municipio   84      183        72         111  \n",
            "277091  Yauco Municipio   85     1110       334         776  \n",
            "\n",
            "[277092 rows x 11 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQEAdvFJ0C5M",
        "outputId": "b94fbc3e-65ca-4722-e3a2-0821aa0817c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://drive.usercontent.google.com/download?id=13JfRr7bFxCt0WYW3qm5XH-jwyEMc-jOj&authuser=0&export=download&confirm=t\n",
            "     STATEFP COUNTYFP  COUNTYNS  GEOID         GEOIDFQ     NAME  \\\n",
            "1323      01      001  00161526  01001  0500000US01001  Autauga   \n",
            "1242      01      003  00161527  01003  0500000US01003  Baldwin   \n",
            "1829      01      005  00161528  01005  0500000US01005  Barbour   \n",
            "3016      01      007  00161529  01007  0500000US01007     Bibb   \n",
            "853       01      009  00161530  01009  0500000US01009   Blount   \n",
            "\n",
            "            NAMELSAD LSAD CLASSFP  MTFCC CSAFP CBSAFP METDIVFP FUNCSTAT  \\\n",
            "1323  Autauga County   06      H1  G4020   388  33860     None        A   \n",
            "1242  Baldwin County   06      H1  G4020   380  19300     None        A   \n",
            "1829  Barbour County   06      H1  G4020  None  21640     None        A   \n",
            "3016     Bibb County   06      H1  G4020   142  13820     None        A   \n",
            "853    Blount County   06      H1  G4020   142  13820     None        A   \n",
            "\n",
            "           ALAND      AWATER     INTPTLAT      INTPTLON  \\\n",
            "1323  1539631459    25677536  +32.5322367  -086.6464395   \n",
            "1242  4117781416  1132830835  +30.6592183  -087.7460666   \n",
            "1829  2292160151    50523213  +31.8702531  -085.4051035   \n",
            "3016  1612188713     9572302  +33.0158929  -087.1271475   \n",
            "853   1670259099    14860281  +33.9773575  -086.5664400   \n",
            "\n",
            "                                               geometry State  \n",
            "1323  POLYGON ((-86.58826 32.36775, -86.58834 32.367...    AL  \n",
            "1242  POLYGON ((-87.97692 31.08658, -87.97688 31.087...    AL  \n",
            "1829  POLYGON ((-85.41585 31.68164, -85.41619 31.677...    AL  \n",
            "3016  POLYGON ((-86.87657 33.01891, -86.87657 33.018...    AL  \n",
            "853   POLYGON ((-86.56421 33.80194, -86.56556 33.801...    AL  \n",
            "State  NAME      \n",
            "AL     Autauga       01001\n",
            "       Baldwin       01003\n",
            "       Barbour       01005\n",
            "       Bibb          01007\n",
            "       Blount        01009\n",
            "                     ...  \n",
            "PR     Yabucoa       72151\n",
            "       Yauco         72153\n",
            "VI     St. Croix     78010\n",
            "       St. John      78020\n",
            "       St. Thomas    78030\n",
            "Name: GEOID, Length: 3235, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Function to download, extract, and load U.S. county boundaries data using Census TIGER/Line Shapefiles.\n",
        "def load_county_boundaries():\n",
        "    \"\"\"\n",
        "    From the Census TIGER/Line Shapefiles\n",
        "    Page: https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html\n",
        "    Datasets: https://www2.census.gov/geo/tiger/TIGER2024/COUNTY/\n",
        "    Filename: tl_2024_us_county.zip\n",
        "    url for direct download: https://www2.census.gov/geo/tiger/TIGER2024/COUNTY/tl_2024_us_county.zip\n",
        "    \"\"\"\n",
        "    file_id = '13JfRr7bFxCt0WYW3qm5XH-jwyEMc-jOj'\n",
        "    file_url = google_drive_file_download_url.substitute(file_id=file_id)\n",
        "\n",
        "    # Need to use a different endpoint and add `confirm=t` query param to download \"can't scan for viruses\" files\n",
        "    file_url = file_url.replace('uc?', 'download?') + '&confirm=t'\n",
        "    print(file_url)\n",
        "\n",
        "    extraction_path = os.path.join('content', 'tl_2024_us_county')\n",
        "    response = requests.get(file_url)\n",
        "    # It's necessary to extract the files into the (temporary) colab working directory because geopandas needs to see all the files at once\n",
        "    with zipfile.ZipFile(io.BytesIO(response.content), 'r') as zf:\n",
        "        zf.extractall(extraction_path)\n",
        "\n",
        "    # Find the (unique) filename for the main shapefile from the extracted files\n",
        "    shp_fn = next(fn for fn in os.listdir(extraction_path) if fn.endswith('.shp'))\n",
        "    # Combine the extraction_path and the shp_filename into a shp_filepath\n",
        "    shp_fp = os.path.join(extraction_path, shp_fn)\n",
        "\n",
        "    gdf = gpd.read_file(shp_fp).sort_values(\"GEOID\")\n",
        "    # Convert the state FIPS codes in the \"STATEFP\" column to state abbreviations using the dictionary `FIPS_to_abbreviations`\n",
        "    # This adds a new column called \"State\" with the corresponding state abbreviations\n",
        "    gdf[\"State\"] = gdf[\"STATEFP\"].apply(FIPS_to_abbreviations.get)\n",
        "    return gdf\n",
        "\n",
        "gdf_COUNTY = load_county_boundaries()\n",
        "gdf_COUNTY.drop(columns='geometry').to_csv('content/counties_without_geometry.csv')\n",
        "print(gdf_COUNTY.head())\n",
        "Name_to_GEOID = gdf_COUNTY.set_index([\"State\", \"NAME\"]).GEOID\n",
        "print(Name_to_GEOID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCryeGLyWdP4",
        "outputId": "10ed6ff4-67f3-4153-96f2-152847733028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  REGION DIVISION STATEFP   STATENS GEOID      GEOIDFQ STUSPS           NAME  \\\n",
            "0      3        5      54  01779805    54  0400000US54     WV  West Virginia   \n",
            "1      3        5      12  00294478    12  0400000US12     FL        Florida   \n",
            "2      2        3      17  01779784    17  0400000US17     IL       Illinois   \n",
            "3      2        4      27  00662849    27  0400000US27     MN      Minnesota   \n",
            "4      3        5      24  01714934    24  0400000US24     MD       Maryland   \n",
            "\n",
            "  LSAD  MTFCC FUNCSTAT         ALAND       AWATER     INTPTLAT      INTPTLON  \\\n",
            "0   00  G4000        A   62266513826    488918898  +38.6472854  -080.6183274   \n",
            "1   00  G4000        A  138965379385  45968913048  +28.3989775  -082.5143005   \n",
            "2   00  G4000        A  143778206717   6216848695  +40.1028754  -089.1526108   \n",
            "3   00  G4000        A  206244791203  18937236061  +46.3159573  -094.1996043   \n",
            "4   00  G4000        A   25151223822   6979843236  +38.9466584  -076.6744939   \n",
            "\n",
            "                                            geometry State  \n",
            "0  POLYGON ((-77.75438 39.33346, -77.75422 39.333...    WV  \n",
            "1  MULTIPOLYGON (((-83.10874 24.62949, -83.10711 ...    FL  \n",
            "2  POLYGON ((-87.89243 38.28285, -87.89334 38.282...    IL  \n",
            "3  POLYGON ((-95.31991 48.99892, -95.31778 48.998...    MN  \n",
            "4  POLYGON ((-75.756 39.24607, -75.75578 39.24334...    MD  \n"
          ]
        }
      ],
      "source": [
        "# Function to download, extract, and load U.S. state boundaries data using Census TIGER/Line Shapefiles.\n",
        "def load_state_boundaries():\n",
        "    \"\"\"\n",
        "    From the Census TIGER/Line Shapefiles\n",
        "    Page: https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html\n",
        "    Datasets: https://www2.census.gov/geo/tiger/TIGER2024/COUNTY/\n",
        "    Filename: tl_2024_us_county.zip\n",
        "    url for direct download: https://www2.census.gov/geo/tiger/TIGER2024/COUNTY/tl_2024_us_county.zip\n",
        "    \"\"\"\n",
        "    file_id = '1vbfbBcl5ZH-JGFd9FrCV9x6arl2m-Rth'\n",
        "    file_url = google_drive_file_download_url.substitute(file_id=file_id)\n",
        "\n",
        "    extraction_path = os.path.join('content', 'tl_2024_us_state')\n",
        "    response = requests.get(file_url)\n",
        "    # It's necessary to extract the files into the (temporary) colab working directory because geopandas needs to see all the files at once\n",
        "    with zipfile.ZipFile(io.BytesIO(response.content), 'r') as zf:\n",
        "        zf.extractall(extraction_path)\n",
        "\n",
        "    # Find the (unique) filename for the main shapefile from the extracted files\n",
        "    shp_fn = next(fn for fn in os.listdir(extraction_path) if fn.endswith('.shp'))\n",
        "    # Combine the extraction_path and the shp_filename into a shp_filepath\n",
        "    shp_fp = os.path.join(extraction_path, shp_fn)\n",
        "\n",
        "    gdf = gpd.read_file(shp_fp)\n",
        "    gdf[\"State\"] = gdf[\"STATEFP\"].apply(FIPS_to_abbreviations.get)\n",
        "    return gdf\n",
        "\n",
        "gdf_STATE = load_state_boundaries()\n",
        "print(gdf_STATE.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF9U5p2RWXSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a351278a-aa7b-45b6-f8a7-e759999151dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   STATE_NAME  Registration Count State\n",
            "0     Alabama               13047    AL\n",
            "1      Alaska                2697    AK\n",
            "2     Arizona               89798    AZ\n",
            "3    Arkansas                7108    AR\n",
            "4  California             1256646    CA\n"
          ]
        }
      ],
      "source": [
        "# Function to download and load electric vehicle registration data by state from the U.S. Department of Energy.\n",
        "def load_registrations_state_level():\n",
        "    \"\"\"\n",
        "    Electric Vehicle Registrations by State by the Alternative Fuels Data Center (U.S. Department of Energy)\n",
        "    The specific dataset is https://afdc.energy.gov/data/10962\n",
        "    The list of datasets is at https://afdc.energy.gov/data/categories/maps-data-categories?per_page=150\n",
        "\n",
        "    Filename:\n",
        "    10962-ev-registration-counts-by-state_9-06-24.xlsx\n",
        "\n",
        "    URL for direct download:\n",
        "    https://afdc.energy.gov/files/u/data/data_source/10962/10962-ev-registration-counts-by-state_9-06-24.xlsx?12518e7893\n",
        "    \"\"\"\n",
        "    file_id = \"1glCUH55GRLn-WCp3QIEoToyEYpXwpsmi\"\n",
        "    file_url = google_drive_file_download_url.substitute(file_id=file_id)\n",
        "\n",
        "    # Read the Excel file into a pandas DataFrame\n",
        "    # - skiprows=2: Skips the first two rows to get to the data\n",
        "    # - usecols=[1, 2]: Loads only the relevant columns (State name and registration count)\n",
        "    df = pd.read_excel(file_url, skiprows=2, usecols=[1,2])\n",
        "    # Rename the column 'State' to 'STATE_NAME' for consistency with other datasets\n",
        "    df.rename(columns = {\"State\": \"STATE_NAME\"}, inplace=True)\n",
        "    return df\n",
        "\n",
        "registrations_state_df = load_registrations_state_level()\n",
        "# Map state names to their abbreviations using the 'states_to_abbreviations' dictionary\n",
        "registrations_state_df[\"State\"] = registrations_state_df[\"STATE_NAME\"].apply(states_to_abbreviations.get)\n",
        "print(registrations_state_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_registrations_county_level():\n",
        "    \"\"\"\n",
        "    Electric Vehicle Registrations by County in 18 states, by Atlas EV Hub\n",
        "    https://www.atlasevhub.com/materials/state-ev-registration-data/\n",
        "\n",
        "    The script to download all 18 datasets individually and compile the data into\n",
        "    a single aggregated .csv dataset is saved in our project folder as build_atlasevhub.py\n",
        "    https://drive.google.com/file/d/1rpRLxOLxBxKv_zI64h42_2WQfkYDQ0Cx/view?usp=drive_link\n",
        "\n",
        "    The file created by the script is named: Atlas_EV_Hub_EV_Registrations.csv\n",
        "    and is saved in our DATASETS folder\n",
        "    \"\"\"\n",
        "    file_id = \"15lj2RGEuGtN32pbPvbcM-5wUj_uNcqQ2\"\n",
        "    file_url = google_drive_file_download_url.substitute(file_id=file_id)\n",
        "\n",
        "    df = pd.read_csv(file_url, dtype={\n",
        "        \"County GEOID\": str,\n",
        "        \"ZIP Code\": str,\n",
        "        \"County\": str,})\n",
        "    df[\"State\"] = df[\"State\"].apply(states_to_abbreviations.get)\n",
        "\n",
        "    # hot-fixes:\n",
        "    df['County'] = df['County'].str.replace(' County$', '', regex=True)\n",
        "    df['County'] = df['County'].str.replace(' city$', '', regex=True)\n",
        "    df.drop(df[df['County'].isin(['Other', 'Unknown'])].index, inplace=True)\n",
        "    df.loc[(df['State'] == 'FL') & (df['County'] == 'Dade'), 'County'] = 'Miami-Dade'\n",
        "    df.loc[df['County'] == 'Desoto', 'County'] = 'DeSoto'\n",
        "    df.loc[(df['State'] == 'FL') & (df['County'] == 'Gadsen'), 'County'] = 'Gadsden'\n",
        "    df[\"County GEOID\"] = df.apply(\n",
        "        lambda row: row[\"County GEOID\"] if pd.isna(row[\"County\"]) else Name_to_GEOID.loc[(row[\"State\"], row[\"County\"])].iloc[0],\n",
        "        axis = 1 )\n",
        "    df.drop(columns=\"County\", inplace=True)\n",
        "\n",
        "    df = df.merge(\n",
        "        right = ZipCode_FIPS_df[[\"ZIP\", \"COUNTY\", \"TOT_RATIO\"]],\n",
        "        how=\"left\",\n",
        "        left_on=\"ZIP Code\", right_on=\"ZIP\",\n",
        "    )\n",
        "    # Eliminate the portions of ZIP codes that are in states outside of the registering agency:\n",
        "    df = df[\n",
        "        pd.notna(df[\"County GEOID\"])\n",
        "        | (df[\"State\"] == df[\"COUNTY\"].apply(gdf_COUNTY.set_index(\"GEOID\").State.get))\n",
        "    ]\n",
        "\n",
        "    df['FIPS'] = df['County GEOID'].combine_first(df['COUNTY'])\n",
        "    df[\"EV Registrations\"] = df[\"EV Registrations\"] * df[\"TOT_RATIO\"].fillna(1.0)\n",
        "    df.drop(columns=[\"County GEOID\", \"ZIP Code\", \"ZIP\", \"COUNTY\", \"TOT_RATIO\"], inplace=True)\n",
        "\n",
        "    # Before aggregating all registration years together:\n",
        "    df = df.groupby([\"Registration Year\", \"FIPS\"])[\"EV Registrations\"]\\\n",
        "        .sum().reset_index().sort_values([\"FIPS\", \"Registration Year\"])\n",
        "    print(df)\n",
        "\n",
        "    # After aggregating all registration years together:\n",
        "    df = df.groupby(\"FIPS\")[\"EV Registrations\"]\\\n",
        "        .sum().sort_index().reset_index().set_index(\"FIPS\")\n",
        "    df[\"State\"] = gdf_COUNTY.set_index(\"GEOID\").State\n",
        "    df[\"County Name\"] = gdf_COUNTY.set_index(\"GEOID\").NAME\n",
        "    df[\"Actual Registrations in State\"] = df[\"State\"].apply(\n",
        "        registrations_state_df.set_index(\"State\")[\"Registration Count\"].get)\n",
        "    df.dropna(subset=\"Actual Registrations in State\", inplace=True)\n",
        "    df[\"Aggregated Registrations in State\"] = df[\"State\"].apply(\n",
        "        df.groupby(\"State\")[\"EV Registrations\"].sum().get )\n",
        "    print(df)\n",
        "    df[\"EV Registrations\"] = df[\"EV Registrations\"] * (df[\"Actual Registrations in State\"] / df[\"Aggregated Registrations in State\"])\n",
        "    df[\"EV Registrations\"] = df[\"EV Registrations\"].round().astype(int)\n",
        "    df = df[[\"EV Registrations\", \"State\", \"County Name\"]]\n",
        "    print(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "registrations_county_df = load_registrations_county_level()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPwDHmHvvLmV",
        "outputId": "ff76c447-9c3e-4a66-9c97-3097197ad344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-29e0689ee079>:30: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
            "  lambda row: row[\"County GEOID\"] if pd.isna(row[\"County\"]) else Name_to_GEOID.loc[(row[\"State\"], row[\"County\"])].iloc[0],\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Registration Year   FIPS  EV Registrations\n",
            "76                 2010  06001         20.000000\n",
            "181                2011  06001        362.000000\n",
            "389                2012  06001       1168.000000\n",
            "641                2013  06001       3622.000000\n",
            "923                2014  06001       8304.000000\n",
            "...                 ...    ...               ...\n",
            "3602               2018  55141         46.167064\n",
            "4626               2019  55141         22.150042\n",
            "5793               2020  55141         29.791379\n",
            "6924               2021  55141         37.588877\n",
            "7991               2022  55141         47.474578\n",
            "\n",
            "[9706 rows x 3 columns]\n",
            "       EV Registrations State County Name  Actual Registrations in State  \\\n",
            "FIPS                                                                       \n",
            "06001     192172.000000    CA     Alameda                        1256646   \n",
            "06003         38.000000    CA      Alpine                        1256646   \n",
            "06005        768.000000    CA      Amador                        1256646   \n",
            "06007       3423.000000    CA       Butte                        1256646   \n",
            "06009        999.000000    CA   Calaveras                        1256646   \n",
            "...                 ...   ...         ...                            ...   \n",
            "55133       2428.866463    WI    Waukesha                          24943   \n",
            "55135        133.900483    WI     Waupaca                          24943   \n",
            "55137         32.670446    WI    Waushara                          24943   \n",
            "55139        580.478943    WI   Winnebago                          24943   \n",
            "55141        183.171940    WI        Wood                          24943   \n",
            "\n",
            "       Aggregated Registrations in State  \n",
            "FIPS                                      \n",
            "06001                          2522939.0  \n",
            "06003                          2522939.0  \n",
            "06005                          2522939.0  \n",
            "06007                          2522939.0  \n",
            "06009                          2522939.0  \n",
            "...                                  ...  \n",
            "55133                            23548.0  \n",
            "55135                            23548.0  \n",
            "55137                            23548.0  \n",
            "55139                            23548.0  \n",
            "55141                            23548.0  \n",
            "\n",
            "[1199 rows x 5 columns]\n",
            "       EV Registrations State County Name\n",
            "FIPS                                     \n",
            "06001             95719    CA     Alameda\n",
            "06003                19    CA      Alpine\n",
            "06005               383    CA      Amador\n",
            "06007              1705    CA       Butte\n",
            "06009               498    CA   Calaveras\n",
            "...                 ...   ...         ...\n",
            "55133              2573    WI    Waukesha\n",
            "55135               142    WI     Waupaca\n",
            "55137                35    WI    Waushara\n",
            "55139               615    WI   Winnebago\n",
            "55141               194    WI        Wood\n",
            "\n",
            "[1199 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4hv4MHZYK5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a050ed-826d-4b7c-f807-23bb00e0e767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-842a7995640b>:24: DtypeWarning: Columns (3,6,20,31,45,46,54,63,69) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  with zf.open(csv_fn) as f: df = pd.read_csv(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Fuel Type Code', 'Station Name', 'Street Address',\n",
            "       'Intersection Directions', 'City', 'State', 'ZIP', 'Plus4',\n",
            "       'Station Phone', 'Status Code', 'Expected Date',\n",
            "       'Groups With Access Code', 'Access Days Time', 'Cards Accepted',\n",
            "       'BD Blends', 'NG Fill Type Code', 'NG PSI', 'EV Level1 EVSE Num',\n",
            "       'EV Level2 EVSE Num', 'EV DC Fast Count', 'EV Other Info', 'EV Network',\n",
            "       'EV Network Web', 'Geocode Status', 'Latitude', 'Longitude',\n",
            "       'Date Last Confirmed', 'ID', 'Updated At', 'Owner Type Code',\n",
            "       'Federal Agency ID', 'Federal Agency Name', 'Open Date',\n",
            "       'Hydrogen Status Link', 'NG Vehicle Class', 'LPG Primary',\n",
            "       'E85 Blender Pump', 'EV Connector Types', 'Country',\n",
            "       'Intersection Directions (French)', 'Access Days Time (French)',\n",
            "       'BD Blends (French)', 'Groups With Access Code (French)',\n",
            "       'Hydrogen Is Retail', 'Access Code', 'Access Detail Code',\n",
            "       'Federal Agency Code', 'Facility Type', 'CNG Dispenser Num',\n",
            "       'CNG On-Site Renewable Source', 'CNG Total Compression Capacity',\n",
            "       'CNG Storage Capacity', 'LNG On-Site Renewable Source',\n",
            "       'E85 Other Ethanol Blends', 'EV Pricing', 'EV Pricing (French)',\n",
            "       'LPG Nozzle Types', 'Hydrogen Pressures', 'Hydrogen Standards',\n",
            "       'CNG Fill Type Code', 'CNG PSI', 'CNG Vehicle Class',\n",
            "       'LNG Vehicle Class', 'EV On-Site Renewable Source', 'Restricted Access',\n",
            "       'RD Blends', 'RD Blends (French)', 'RD Blended with Biodiesel',\n",
            "       'RD Maximum Biodiesel Level', 'NPS Unit Name',\n",
            "       'CNG Station Sells Renewable Natural Gas',\n",
            "       'LNG Station Sells Renewable Natural Gas'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Function to download and load Electric Vehicle (EV) fuel station data from a Kaggle dataset.\n",
        "def load_fuel_stations():\n",
        "    \"\"\"\n",
        "    Electric Vehicle fuel stations 2023.05.18 from kaggle\n",
        "    https://www.kaggle.com/datasets/torinsrose1/ev-fuel-stations-usa-2023-5-18/data\n",
        "\n",
        "    Filename:\n",
        "    ev-fuel-stations-usa-2023-5-18.zip\n",
        "\n",
        "    URL for direct download:\n",
        "    https://www.kaggle.com/api/v1/datasets/download/torinsrose1/ev-fuel-stations-usa-2023-5-18\n",
        "    \"\"\"\n",
        "    file_id = \"1w1Midox6QTxCM3-7GhweerVpgNX9iSER\"\n",
        "    file_url = google_drive_file_download_url.substitute(file_id=file_id)\n",
        "    # Define the CSV filename within the downloaded ZIP archive\n",
        "    csv_fn = \"ev_fuel_stations_5.18.2023.csv\"\n",
        "\n",
        "\n",
        "    # Send an HTTP GET request to download the ZIP file from Google Drive\n",
        "    response = requests.get(file_url)\n",
        "    # Open the downloaded content as a ZIP file using 'io.BytesIO'\n",
        "    with zipfile.ZipFile(io.BytesIO(response.content), 'r') as zf:\n",
        "        # Extract the CSV file from the ZIP archive and load it into a pandas DataFrame\n",
        "        with zf.open(csv_fn) as f: df = pd.read_csv(f)\n",
        "    return df\n",
        "\n",
        "fuel_stations_df = load_fuel_stations()\n",
        "geometry = gpd.points_from_xy(fuel_stations_df['Longitude'], fuel_stations_df['Latitude'])\n",
        "fuel_stations_gdf = gpd.GeoDataFrame(fuel_stations_df, geometry=geometry, crs=\"EPSG:4269\")\n",
        "fuel_stations_gdf[\"FIPS County\"] = gpd.sjoin(fuel_stations_gdf, gdf_COUNTY, how=\"left\", predicate=\"intersects\").GEOID\n",
        "\n",
        "print(fuel_stations_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixsnz9eSZjmU"
      },
      "outputs": [],
      "source": [
        "# Function to download and process data on the highest level of education attainment from the US Department of Agriculture.\n",
        "def load_completing_college():\n",
        "    \"\"\"\n",
        "    Highest level of education attainmeent from US Department of Agriculture\n",
        "    Cannot be downloaded from a stable url (requires active session with cookies)\n",
        "\n",
        "    https://data.ers.usda.gov/reports.aspx?ID=17829\n",
        "\n",
        "    Filename:\n",
        "    EducationReport.xlsx\n",
        "    \"\"\"\n",
        "    file_id = \"1uSDfddUmGbR7H13OhdswxxJKyC_3nWZv\"\n",
        "    file_url = google_drive_file_download_url.substitute(file_id=file_id)\n",
        "\n",
        "    # Read the Excel file into a pandas DataFrame\n",
        "    # - skiprows=2: Skips the first two rows which may contain metadata or headers\n",
        "    # - header=[0,1]: Indicates that the dataset has multi-level column headers\n",
        "    # - index_col=0: Uses the first column as the index (state names)\n",
        "    df = pd.read_excel(file_url, skiprows=2, header=[0,1], index_col=0)\n",
        "    # Flatten multi-level column headers by joining levels with an underscore\n",
        "    df.columns = ['_'.join(map(str, col)).strip() for col in df.columns]\n",
        "    # Remove the row corresponding to \"United States\" to focus on state-level data\n",
        "    df = df[df.index != \"United States\"]\n",
        "    # Map state names to their abbreviations using the 'states_to_abbreviations' dictionary\n",
        "    # This renames the index to use state abbreviations instead of full state names\n",
        "    df.rename(index=states_to_abbreviations.get, inplace=True)\n",
        "    return df\n",
        "\n",
        "# Function to extract the 'Total_2018-2022' column from the processed education data.\n",
        "# This represents the percentage of the population completing higher education for each state.\n",
        "def completing_college_by_state():\n",
        "    college_df = load_completing_college()\n",
        "    return college_df['Total_2018-2022']\n",
        "\n",
        "college_Series = completing_college_by_state()\n",
        "print(college_Series.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Hi61iVOycnP"
      },
      "outputs": [],
      "source": [
        "#  Function to download and process personal mean income data by county from the U.S. Bureau of Economic Analysis (BEA).\n",
        "def load_mean_income():\n",
        "    \"\"\"\n",
        "    Personal (Mean) Income by County\n",
        "    https://www.bea.gov/data/income-saving/personal-income-county-metro-and-other-areas\n",
        "\n",
        "    Filename:\n",
        "    lapi1123.xlsx\n",
        "\n",
        "    URL for direct download:\n",
        "    https://www.bea.gov/data/income-saving/personal-income-county-metro-and-other-areas\n",
        "    \"\"\"\n",
        "    file_id = \"1R_jxZgjuJy9Enf9eJOIyV0dTy8B5Vd01\"\n",
        "    file_url = google_drive_file_download_url.substitute(file_id=file_id)\n",
        "    df = pd.read_excel(file_url, skiprows=2, header=[0,1])\n",
        "    df.columns = ['_'.join(map(str, col)).strip() for col in df.columns]\n",
        "    df.columns.values[0] = \"NAME\"\n",
        "    df.dropna(how=\"all\", inplace=True)\n",
        "    df = df.iloc[0:-3]\n",
        "    current_state = None\n",
        "    for idx, row in df.iterrows():\n",
        "        if row['Rank in state_2022'] == '--': current_state = row[\"NAME\"]\n",
        "        df.loc[idx, \"State\"] = current_state\n",
        "    df[\"State\"] = df[\"State\"].apply(states_to_abbreviations.get)\n",
        "\n",
        "    states_df  = df[(df[\"Rank in state_2022\"] == '--') & (df[\"NAME\"] != \"United States\")]\n",
        "    states_df = states_df.reset_index().drop(\"index\", axis=1)\n",
        "\n",
        "    counties_df = df[df[\"Rank in state_2022\"] != '--']\n",
        "    FIPS_codes_df = load_FIPS_codes()\n",
        "\n",
        "    # # hot-fixes:\n",
        "    FIPS_codes_df.loc[FIPS_codes_df['STATE'] == 'VA', 'COUNTYNAME'] = FIPS_codes_df.loc[FIPS_codes_df['STATE'] == 'VA', 'COUNTYNAME'].str.replace( ' city$', '', regex=True)\n",
        "    FIPS_codes_df['COUNTYNAME'] = FIPS_codes_df['COUNTYNAME'].str.replace(' County$', '', regex=True)\n",
        "    FIPS_codes_df['COUNTYNAME'] = FIPS_codes_df['COUNTYNAME'].str.replace(' Parish$', '', regex=True)\n",
        "    FIPS_codes_df['COUNTYNAME'] = FIPS_codes_df['COUNTYNAME'].str.replace(' City$', ' city', regex=True)\n",
        "\n",
        "    counties_df['NAME'] = counties_df['NAME'].str.split(r' \\+').str[0]\n",
        "    counties_df['NAME'] = counties_df['NAME'].str.split(r',').str[0]\n",
        "    counties_df['NAME'] = counties_df['NAME'].str.replace(' City$', ' city', regex=True)\n",
        "    counties_df['NAME'] = counties_df['NAME'].str.replace('Augusta, Staunton + Waynesboro', 'Augusta')\n",
        "    counties_df['NAME'] = counties_df['NAME'].str.replace('Fremont (includes Yellowstone National Park)', 'Fremont')\n",
        "    counties_df['NAME'] = counties_df['NAME'].str.replace('Lagrange', 'LaGrange')\n",
        "    counties_df = counties_df[~counties_df['NAME'].isin(['Independent cities:', 'Combination areas2:', 'St.'])]\n",
        "    Name_to_FIPS = FIPS_codes_df.set_index([\"STATE\", \"COUNTYNAME\"]).FIPS\n",
        "    counties_df[\"FIPS\"] = counties_df.apply(\n",
        "        lambda row: Name_to_FIPS.loc[(row[\"State\"], row[\"NAME\"])].iloc[0],\n",
        "        axis = 1 )\n",
        "\n",
        "    return states_df, counties_df\n",
        "\n",
        "mean_income_df, mean_income_counties_df = load_mean_income()\n",
        "mean_income_Series = mean_income_df.set_index(\"State\")[\"Dollars_2022\"]\n",
        "mean_income_counties_Series = mean_income_counties_df.set_index(\"FIPS\")[\"Dollars_2022\"]\n",
        "print(mean_income_Series.head())\n",
        "print(mean_income_counties_Series.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpU063Fkan9V"
      },
      "source": [
        "# Build ABT (Analytic Base Table):\n",
        "\n",
        "It is important to construct a well-organized and feature-rich ABT, to ensure we are laying the groundwork for more effective, efficient, and insightful data analysis, ultimately leading to better decision-making and predictive modeling outcomes.\n",
        "\n",
        "More information on general ABTs: https://en.wikipedia.org/wiki/Analytical_base_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4X8C-GFNcOcl"
      },
      "outputs": [],
      "source": [
        "# Calculate the total population by state for the year 2023.\n",
        "# Uses the 'load_population_estimates()' function to retrieve data.\n",
        "# - Filters for both sexes (SEX = 0)\n",
        "# - Filters for total population across all age groups (AGE = 999)\n",
        "# - Sets the state abbreviation as the index\n",
        "# - Returns a pandas Series containing total population estimates by state\n",
        "def total_population_by_state():\n",
        "    df = load_population_estimates()\n",
        "    df = df[df[\"SEX\"] == 0].drop(\"SEX\", axis=1) # 0 means both sexes, see codebook\n",
        "    df = df[df[\"AGE\"] == 999].drop(\"AGE\", axis=1)\n",
        "    df = df.set_index(\"State\")\n",
        "    # Return the total population for each state\n",
        "    return df[\"POPEST2023_CIV\"]\n",
        "\n",
        "# Calculate the median age for each state based on cumulative population estimates.\n",
        "# - Filters for both sexes (SEX = 0)\n",
        "# - Excludes the total population row (AGE = 999)\n",
        "# - Sorts data by state and age for accurate cumulative summation\n",
        "# - Calculates the median age using cumulative population interpolation\n",
        "# - Returns a pandas Series with the median age by state\n",
        "def median_age_by_state():\n",
        "    df = load_population_estimates()\n",
        "    df = df[df[\"SEX\"] == 0].drop(\"SEX\", axis=1) # 0 means both sexes, see codebook above\n",
        "    df = df[df[\"AGE\"] != 999] # Drops the row with 999 which indicates the total population\n",
        "    df = df.sort_values([\"State\", \"AGE\"]) #  Sort data by state and age to ensure cumulative sum is calculated correctly\n",
        "    # Calculate the median age for a single state using cumulative population.\n",
        "    # - Creates a cumulative sum of the population by age\n",
        "    # - Identifies the median age using interpolation\n",
        "    def median_age(state):\n",
        "        state[\"cumulative_POP\"] = state[\"POPEST2023_CIV\"].cumsum()\n",
        "        median_person = state[\"POPEST2023_CIV\"].sum() / 2\n",
        "        state.reset_index(inplace=True) # Reset index to ensure it is numeric\n",
        "        # Iterate through rows to find the median age by interpolation\n",
        "        for i, row in state.iterrows():\n",
        "            if row[\"cumulative_POP\"] >= median_person:\n",
        "                age1 = (0 if i == 0 else state.iloc[i-1][\"AGE\"])\n",
        "                age2 = row[\"AGE\"]\n",
        "                pop1 = 0 if i == 0 else state.iloc[i-1][\"cumulative_POP\"]\n",
        "                pop2 = row[\"cumulative_POP\"]\n",
        "                # Calculate interpolative distance to estimate the median age\n",
        "                interpolative_distance = (median_person - pop1) / (pop2 - pop1)\n",
        "                interpolated_age = interpolative_distance * (age2 - age1) + age1\n",
        "                return interpolated_age\n",
        "    # Group by state and apply the median age calculation\n",
        "    return df.groupby('State').apply(median_age)\n",
        "\n",
        "# Calculate the sex ratio (males to females) for each state.\n",
        "# - Filters for total population across all ages (AGE = 999)\n",
        "# - Groups data by state to calculate the ratio of males to females\n",
        "# - Returns a pandas Series with the sex ratio by state\n",
        "def sex_ratio_by_state():\n",
        "    df = load_population_estimates()\n",
        "    df = df[df[\"AGE\"] == 999].drop(\"AGE\", axis=1)\n",
        "    # Calculate the ratio of males to females for a single state.\n",
        "    # - SEX = 1 represents males & SEX = 2 represents females\n",
        "    # - Returns the ratio of males to females\n",
        "    def sex_ratio(state):\n",
        "        males = state[df[\"SEX\"] == 1][\"POPEST2023_CIV\"].iloc[0]\n",
        "        females = state[df[\"SEX\"] == 2][\"POPEST2023_CIV\"].iloc[0]\n",
        "        return males / females\n",
        "    # Group by state and apply the sex ratio calculation\n",
        "    return np.log( df.groupby('State').apply(sex_ratio) )\n",
        "\n",
        "# Load and calculate total population by state\n",
        "population_Series = total_population_by_state()\n",
        "# Load and calculate median age by state\n",
        "median_age_Series = median_age_by_state()\n",
        "# Load and calculate sex ratio by state\n",
        "sex_ratio_Series  = sex_ratio_by_state()\n",
        "# Load and assign integer of accessibility type by state\n",
        "accessibility_Series  = load_fuel_stations()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def total_population_by_county():\n",
        "    df = population_estimates_county_df.copy()\n",
        "    return df.groupby(\"FIPS\").TOT_POP.sum()\n",
        "\n",
        "def median_age_by_county():\n",
        "    df = population_estimates_county_df.copy()\n",
        "    df = df.sort_values([\"FIPS\", \"AGE\"]) #  Sort data by state and age to ensure cumulative sum is calculated correctly\n",
        "    def median_age(county):\n",
        "        county[\"cumulative_POP\"] = county[\"TOT_POP\"].cumsum()\n",
        "        median_person = county[\"TOT_POP\"].sum() / 2\n",
        "        county.reset_index(inplace=True) # Reset index to ensure it is numeric\n",
        "        # Iterate through rows to find the median age by interpolation\n",
        "        for i, row in county.iterrows():\n",
        "            if row[\"cumulative_POP\"] >= median_person:\n",
        "                age1 = (0 if i == 0 else county.iloc[i-1][\"AGE\"])\n",
        "                age2 = row[\"AGE\"]\n",
        "                pop1 = 0 if i == 0 else county.iloc[i-1][\"cumulative_POP\"]\n",
        "                pop2 = row[\"cumulative_POP\"]\n",
        "                # Calculate interpolative distance to estimate the median age\n",
        "                interpolative_distance = (median_person - pop1) / (pop2 - pop1)\n",
        "                interpolated_age = interpolative_distance * (age2 - age1) + age1\n",
        "                return interpolated_age\n",
        "    # Group by state and apply the median age calculatio9n\n",
        "    return df.groupby('FIPS').apply(median_age)\n",
        "\n",
        "def sex_ratio_by_county():\n",
        "    df = population_estimates_county_df.copy()\n",
        "    # Calculate the ratio of males to females for a single state.\n",
        "    # - SEX = 1 represents males & SEX = 2 represents females\n",
        "    # - Returns the ratio of males to females\n",
        "    sum_df = df.groupby(\"FIPS\")[[\"TOT_POP\", \"TOT_MALE\", \"TOT_FEMALE\"]].sum()\n",
        "    return np.log( sum_df[\"TOT_MALE\"] / sum_df[\"TOT_FEMALE\"] )\n",
        "\n",
        "population_County_Series = total_population_by_county()\n",
        "print(population_County_Series)\n",
        "median_age_County_Series = median_age_by_county()\n",
        "print(median_age_County_Series)\n",
        "sex_ratio_County_Series = sex_ratio_by_county()\n",
        "print(sex_ratio_County_Series)\n"
      ],
      "metadata": {
        "id": "kLyhtBGDd2Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4HUcIez5UZx"
      },
      "outputs": [],
      "source": [
        "print(\"Computing county areas\") # Prints a message indicating that the calculation of county areas is beginning\n",
        "gdf_usa_albers_projection = gdf_COUNTY.to_crs('EPSG:5070')\n",
        "gdf_usa_albers_projection[\"Area_km2\"] = gdf_usa_albers_projection.geometry.area / 1e6\n",
        "# Group the data by the \"State\" column and sum the areas of all counties within each state\n",
        "# This step calculates the total area of each state by aggregating the areas of its counties.\n",
        "state_areas = gdf_usa_albers_projection.groupby('State').Area_km2.sum()\n",
        "print(state_areas.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJrlPOk5a3A5"
      },
      "outputs": [],
      "source": [
        "# Creating our own dataset:\n",
        "\n",
        "# Create a DataFrame called \"States_ABT\" with initial columns for state names and EV registration counts\n",
        "# This data comes from the \"registrations_state_df\" DataFrame\n",
        "States_ABT = registrations_state_df[[\"State\", \"Registration Count\"]]\n",
        "# Group the fuel stations data by state to get the count of stations in each state\n",
        "# The \"size()\" method counts the number of occurrences (fuel stations) per state\n",
        "fuel_stations_by_state = fuel_stations_df.groupby(\"State\").size()\n",
        "\n",
        "States_ABT[\"Fuel Station Count\"] = States_ABT[\"State\"].apply(fuel_stations_by_state.get) # Add the count of fuel stations to the \"States_ABT\" DataFrame\n",
        "States_ABT[\"Population\"] = States_ABT[\"State\"].apply(population_Series.get) # Add the total population for each state to the DataFrame\n",
        "States_ABT[\"Median Age\"] = States_ABT[\"State\"].apply(median_age_Series.get) # Add the median age for each state\n",
        "States_ABT[\"log Sex Ratio\"] = States_ABT[\"State\"].apply(sex_ratio_Series.get) # Add the sex ratio (males to females) for each state\n",
        "States_ABT[\"Completing College\"] = States_ABT[\"State\"].apply(college_Series.get) # Add the percentage of people completing college for each state\n",
        "States_ABT[\"Mean Income\"] = States_ABT[\"State\"].apply(mean_income_Series.get) # Add the mean income for each state\n",
        "States_ABT[\"Area_km2\"] = States_ABT[\"State\"].apply(state_areas.get) # Add the total area in square kilometers for each state\n",
        "States_ABT[\"Pop_per_km2\"] = States_ABT[\"Population\"] / States_ABT[\"Area_km2\"] # Calculate the population density (people per square kilometer) for each state\n",
        "States_ABT[\"Fuel Stations per capita\"] = States_ABT[\"Fuel Station Count\"] / States_ABT[\"Population\"] # Calculate the number of fuel stations per capita for each state\n",
        "States_ABT[\"EV Registrations per capita\"] = States_ABT[\"Registration Count\"] / States_ABT[\"Population\"] # Calculate the number of EV registrations per capita for each state\n",
        "\n",
        "States_ABT = States_ABT.iloc[:-1]\n",
        "States_ABT = States_ABT.set_index('State')\n",
        "print(States_ABT.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EV = States_ABT\n",
        "EV.to_csv('EV.csv')\n",
        "from google.colab import files\n",
        "files.download('EV.csv')"
      ],
      "metadata": {
        "id": "9NnavtFE_xWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "County_ABT = registrations_county_df[[\"EV Registrations\", \"State\"]]\n",
        "County_ABT[\"Fuel Station Count\"] = County_ABT.index.map(fuel_stations_gdf.groupby(\"FIPS County\").size().get).fillna(0).astype(int)\n",
        "County_ABT[\"Population\"] = County_ABT.index.map(population_County_Series.get)\n",
        "County_ABT[\"Median Age\"] = County_ABT.index.map(median_age_County_Series.get)\n",
        "County_ABT[\"log Sex Ratio\"] = County_ABT.index.map(sex_ratio_County_Series.get)\n",
        "County_ABT[\"Completing College\"] = County_ABT[\"State\"].apply(college_Series.get) # Add the percentage of people completing college for each state\n",
        "\n",
        "County_ABT[\"Mean Income\"] = County_ABT.index.map(mean_income_counties_Series.get) # Add the mean income for each county\n",
        "County_ABT = County_ABT[pd.to_numeric(County_ABT['Mean Income'], errors='coerce').notna()]\n",
        "County_ABT[\"Mean Income\"] = County_ABT[\"Mean Income\"].astype(float)\n",
        "\n",
        "County_ABT[\"Area_km2\"] = County_ABT.index.map(gdf_usa_albers_projection.set_index(\"GEOID\")[\"Area_km2\"].get)\n",
        "County_ABT[\"Pop_per_km2\"] = County_ABT[\"Population\"] / County_ABT[\"Area_km2\"] # Calculate the population density (people per square kilometer) for each county\n",
        "County_ABT[\"Fuel Stations per capita\"] = County_ABT[\"Fuel Station Count\"] / County_ABT[\"Population\"] # Calculate the number of fuel stations per capita for each county\n",
        "County_ABT[\"EV Registrations per capita\"] = County_ABT[\"EV Registrations\"] / County_ABT[\"Population\"] # Calculate the number of EV registrations per capita for each county\n",
        "\n",
        "County_ABT.drop(columns=\"State\", inplace=True)\n",
        "County_ABT.dropna(inplace=True)\n",
        "print(County_ABT)"
      ],
      "metadata": {
        "id": "Gix_iWos03vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbfWy3-mnMfR"
      },
      "source": [
        "# Descriptive Statistics & Other\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "States_ABT.describe()"
      ],
      "metadata": {
        "id": "276Mj-YJSWSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp01w6Q89NgZ"
      },
      "outputs": [],
      "source": [
        "print( States_ABT.info() ) # .info() method in pandas provides a summary of the DataFrame - structure & contents\n",
        "print( County_ABT.info() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-r99zeanYFa"
      },
      "outputs": [],
      "source": [
        "print( States_ABT.describe() ) # .describe() provides count, mean, std, min, 25%, 50%, 75%, & max\n",
        "print( County_ABT.describe() ) # .describe() provides count, mean, std, min, 25%, 50%, 75%, & max"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( States_ABT[\"Registration Count\"].sum() )\n",
        "print( County_ABT[\"EV Registrations\"].sum() )"
      ],
      "metadata": {
        "id": "yVLylTgnBrFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y3zIHvaUihQ"
      },
      "source": [
        "# Data Analysis:\n",
        "\n",
        "In the context of our project, which focuses on analyzing electric vehicle (EV) adoption and infrastructure across different states, data analysis plays a key role in identifying the factors that drive EV registrations. By leveraging demographic, economic, and infrastructure data, we aim to uncover patterns that can inform strategic decisions, such as where to prioritize the expansion of EV charging stations to maximize their impact.\n",
        "\n",
        "To make our findings more accessible, we present multiple visualizations that showcase critical insights derived from our analysis. For example, we use bar charts to compare EV registrations and charging stations per capita across states, scatter plots to explore the relationship between charging infrastructure and EV adoption, and maps to visualize geographical patterns in EV accessibility. These visualizations not only help us communicate our analysis effectively but also provide actionable insights for stakeholders looking to enhance EV infrastructure and policy planning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPd7N5e5dWjK"
      },
      "outputs": [],
      "source": [
        "# Generates a scatter plot with a regression line to visualize the relationship\n",
        "# between fuel stations per capita and EV registrations per capita in the States_ABT DataFrame\n",
        "sns.regplot(x=\"Fuel Stations per capita\", y=\"EV Registrations per capita\", data=States_ABT)\n",
        "for idx, row in States_ABT.iterrows():\n",
        "    plt.text(\n",
        "        row[\"Fuel Stations per capita\"], row[\"EV Registrations per capita\"], idx,\n",
        "        fontsize=8, ha='right', va='bottom'\n",
        "    )\n",
        "\n",
        "plt.title('Relationship Between Fuel Stations and EV Registrations',\n",
        "          fontsize=16, fontweight='bold', fontname='STIXGeneral')\n",
        "plt.xlabel('Fuel Stations Per Capita', fontsize=14, fontname='STIXGeneral')\n",
        "plt.ylabel('EV Registrations Per Capita', fontsize=14, fontname='STIXGeneral')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.show()\n",
        "\n",
        "# The result allows us to see outliers - like California.\n",
        "# The line has an overall positive slope, indicating a positive correlation, suggesting\n",
        "# that states with more fuel stations per capita tend to have higher EV adoption."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.regplot(x=\"Fuel Stations per capita\", y=\"EV Registrations per capita\", data=County_ABT)\n",
        "\n",
        "plt.title('Relationship Between Fuel Stations and EV Registrations',\n",
        "          fontsize=16, fontweight='bold', fontname='STIXGeneral')\n",
        "plt.xlabel('Fuel Stations Per Capita', fontsize=14, fontname='STIXGeneral')\n",
        "plt.ylabel('EV Registrations Per Capita', fontsize=14, fontname='STIXGeneral')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bVXD8lJtA6EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVgF2ph3XyxV"
      },
      "outputs": [],
      "source": [
        "# This code creates a map visualization (geographic boundary map) to show the locations of fuel stations across\n",
        "# the United States and plots the fuel station locations on top of it using latitude and longitude coordinates.\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Plot the state boundaries from the GeoDataFrame\n",
        "gdf_STATE.boundary.plot(ax=ax, linewidth=1, edgecolor='black')\n",
        "\n",
        "# Plot latitude and longitude points\n",
        "public_stations  = fuel_stations_df[fuel_stations_df[\"Groups With Access Code\"].str.contains(\"Public\")]\n",
        "private_stations = fuel_stations_df[fuel_stations_df[\"Groups With Access Code\"].str.contains(\"Private\")]\n",
        "plt.scatter(public_stations['Longitude'], public_stations['Latitude'], color='red', marker='o', s=1, label='Public EV fuel stations', alpha=.2)\n",
        "plt.scatter(private_stations['Longitude'], private_stations['Latitude'], color='blue', marker='o', s=2, label='Private EV fuel stations', alpha=1)\n",
        "\n",
        "ax.set_xlim(-180, -60)\n",
        "ax.set_ylim(15, 75)\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title(\"EV Fuel Stations Over U.S. States Wireframe\", fontsize=20, fontweight='bold', fontname='STIXGeneral')\n",
        "plt.xlabel(\"Longitude\", fontsize=15, fontname='STIXGeneral')\n",
        "plt.ylabel(\"Latitude\", fontsize=15, fontname='STIXGeneral')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# The result of this code allows us to visually assess geographic coverage and identify areas with high or low densities of fuel stations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPZ5I5qSEYO6"
      },
      "outputs": [],
      "source": [
        "# This code creates a BAR PLOT to visualize the Electric Vehicle (EV) registrations per capita for each state\n",
        "# using the data from the States_ABT DataFrame. The visualization highlights how EV adoption varies across different states.\n",
        "\n",
        "evR = States_ABT.reset_index()[['State', 'EV Registrations per capita']]\n",
        "evR.plot(kind='bar', stacked=True, figsize=(15, 8), color='pink', edgecolor='black') # Makes it pretty\n",
        "plt.title('Electric Vehicle Registrations by State', size=25, fontweight='bold', fontname='STIXGeneral')\n",
        "plt.xlabel('State', fontsize=20, labelpad=20, fontname='STIXGeneral')\n",
        "plt.ylabel('EV Registration Per Capita', fontsize=20, labelpad=20, fontname='STIXGeneral')\n",
        "plt.xticks(ticks=np.arange(len(evR['State'])), labels=evR['State'], fontsize=10, fontname='STIXGeneral')\n",
        "plt.yticks(fontsize=15, fontname='STIXGeneral')\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.legend(['EV Registrations Per Capita'], prop={'family': 'STIXGeneral', 'size': 12})\n",
        "\n",
        "# The result identifies states with with high or low EV adoption rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmLELDbcjnKH"
      },
      "outputs": [],
      "source": [
        "# This code generates a bar plot to visualize the number of electric vehicle (EV) fuel stations per\n",
        "# capita across various states. It uses data from the States_ABT DataFrame to show how fuel\n",
        "# station accessibility varies from state to state, which can be critical for understanding infrastructure readiness for EV adoption.\n",
        "\n",
        "evR = States_ABT.reset_index()[['State', 'Fuel Stations per capita']]\n",
        "evR.plot(kind='bar', stacked=True, figsize=(15, 8), color='pink', edgecolor='black') # Makes it pretty)\n",
        "plt.title('Electric Vehicle Stations by State', size=25, fontweight='bold', fontname='STIXGeneral')\n",
        "plt.xlabel('State', fontsize=20, labelpad=20, fontname='STIXGeneral')\n",
        "plt.ylabel('Fuel Stations Per Capita', fontsize=20, labelpad=20, fontname='STIXGeneral')\n",
        "plt.xticks(ticks=np.arange(len(evR['State'])), labels=evR['State'], fontsize=10, fontname='STIXGeneral')\n",
        "plt.yticks(fontsize=15, fontname='STIXGeneral')\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.legend(['Fuel Stations Per Capita'], prop={'family': 'STIXGeneral', 'size': 12})\n",
        "\n",
        "# The result identifies states with high or low accessibility to EV charging stations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR-0_RRSHU3n"
      },
      "outputs": [],
      "source": [
        "# This code creates a HORIZONTAL BOX PLOT that visualizes the distribution of the Fuel Station Count per state.\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.boxplot(States_ABT['Fuel Station Count'].dropna(),\n",
        "            patch_artist=True,\n",
        "            vert=False,\n",
        "            boxprops=dict(facecolor='pink'),\n",
        "            medianprops=dict(color='green'))\n",
        "\n",
        "plt.yticks([1], ['Fuel Station Count'], fontname='STIXGeneral', fontsize=14)\n",
        "plt.xlabel('Fuel Stations Per Capita', fontname='STIXGeneral', fontsize=14)\n",
        "plt.title('Fuel Station Count', fontsize=18, fontweight='bold', fontname='STIXGeneral')\n",
        "plt.grid(True, axis='x')\n",
        "plt.show()\n",
        "\n",
        "# The result gives a visual summary of how evenly or unevenly fuel stations are distributed among the states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQmXBkXMQX79"
      },
      "outputs": [],
      "source": [
        "# This code generates a HISTOGRAM to visualize the distribution of fuel stations per capita across different states\n",
        "# using the data from the States_ABT DataFrame\n",
        "\n",
        "evH = States_ABT['Fuel Stations per capita']\n",
        "plt.hist(evH, bins=30, color='pink', edgecolor='black') # Makes the graph prettier\n",
        "plt.title('Fuel Stations Per Capita', fontsize=14, fontname='STIXGeneral', fontweight='bold')\n",
        "plt.xlabel('Value', fontsize=14, labelpad=12, fontname='STIXGeneral')\n",
        "plt.ylabel('Number of States', fontsize=14, labelpad=12, fontname='STIXGeneral')\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.xticks(fontsize=12, fontname='STIXGeneral')\n",
        "plt.yticks(fontsize=12, fontname='STIXGeneral')\n",
        "\n",
        "plt.axvline(evH.mean(), color='purple', linestyle='dashed', linewidth=1, label='Mean') # Added the Mean function\n",
        "plt.axvline(evH.median(), color='green', linestyle='dashed', linewidth=1, label='Median') # Added the Median function\n",
        "plt.legend()\n",
        "\n",
        "# The result shows the frequency distribution of fuel stations per capita across all states.\n",
        "# This shows whether most states have high or low access to fuel stations relative to their population.\n",
        "# If there are bars that are far away from the rest of the distribution, they indicate states\n",
        "# that are outliers in terms of fuel station availability."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evH = States_ABT['Mean Income']\n",
        "plt.hist(evH, bins=30, color='dodgerblue', edgecolor='black') # Makes the graph prettier\n",
        "plt.title('Mean Income Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Income ($)', fontsize=14, labelpad=12)\n",
        "plt.ylabel('Number of States', fontsize=14, labelpad=12)\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "plt.axvline(evH.mean(), color='purple', linestyle='dashed', linewidth=1, label='Mean') # Added the Mean function\n",
        "plt.axvline(evH.median(), color='green', linestyle='dashed', linewidth=1, label='Median') # Added the Median function\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "3Q-mF9HbPatK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnFUFlBTR6nQ"
      },
      "outputs": [],
      "source": [
        "# This code generates a HISTOGRAM that visualizes the distribution of electric\n",
        "# vehicle (EV) registrations per capita across different states.\n",
        "evH = States_ABT['EV Registrations per capita']\n",
        "plt.hist(evH, bins=30, color='pink', edgecolor='black') # Makes the graph prettier\n",
        "plt.title('EV Registrations Per Capita', fontname='STIXGeneral', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Value', fontsize=14, labelpad=12, fontname='STIXGeneral')\n",
        "plt.ylabel('Number of States', fontsize=14, labelpad=12, fontname='STIXGeneral')\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.xticks(fontsize=12, fontname='STIXGeneral')\n",
        "plt.yticks(fontsize=12, fontname='STIXGeneral')\n",
        "\n",
        "plt.axvline(evH.mean(), color='purple', linestyle='dashed', linewidth=1, label='Mean') # Added the Mean function\n",
        "plt.axvline(evH.median(), color='green', linestyle='dashed', linewidth=1, label='Median') # Added the Median function\n",
        "plt.legend()\n",
        "\n",
        "# The result provides a snapshot of how EV adoption varies across states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUCumgNQTLvD"
      },
      "outputs": [],
      "source": [
        "plt.hist(States_ABT, bins=30, edgecolor='black')\n",
        "plt.title('EV Registrations Per Capita', fontsize=14, fontname='STIXGeneral', fontweight='bold')\n",
        "plt.xlabel('Value', fontsize=14, labelpad=12, fontname='STIXGeneral')\n",
        "plt.ylabel('Number of States', fontsize=14, labelpad=12, fontname='STIXGeneral')\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo4U5KVByB-n"
      },
      "source": [
        "# Build Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHowmJnk9Hv0"
      },
      "outputs": [],
      "source": [
        "def make_best_random_forest_model(X_train, X_test, y_train, y_test):    # Define the model\n",
        "    rf = RandomForestRegressor()\n",
        "\n",
        "    # Set the parameter grid for hyperparameter tuning\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'min_samples_leaf': [1, 2],\n",
        "        'bootstrap': [True, False]\n",
        "    }\n",
        "\n",
        "    # Use GridSearchCV to optimize hyperparameters with cross-validation\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best parameters and model performance\n",
        "    best_params = grid_search.best_params_\n",
        "    best_randomized_forest_model = grid_search.best_estimator_\n",
        "    best_score = -grid_search.best_score_  # Mean squared error\n",
        "\n",
        "    print(\"Best Parameters:\", best_params)\n",
        "    print(\"Best Cross-Validation MSE:\", best_score)\n",
        "\n",
        "    # Evaluate on a test set (if available)\n",
        "    best_randomized_forest_model.fit(X_train, y_train)\n",
        "    y_pred = best_randomized_forest_model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "    print(\"Test MSE:\", test_mse)\n",
        "    return best_randomized_forest_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaEfsxat9Z4P"
      },
      "outputs": [],
      "source": [
        "def make_best_ridge_model(X_train, X_test, y_train, y_test):\n",
        "    ridge = Ridge()\n",
        "\n",
        "    # Set the parameter grid for hyperparameter tuning (e.g., regularization strength)\n",
        "    param_grid = {\n",
        "        'alpha': [0.01, 0.1, 1, 10, 100]\n",
        "    }\n",
        "\n",
        "    # Use GridSearchCV to optimize hyperparameters with cross-validation\n",
        "    grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best parameters and model performance\n",
        "    best_params = grid_search.best_params_\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_score = -grid_search.best_score_  # Mean squared error\n",
        "\n",
        "    print(\"Best Parameters:\", best_params)\n",
        "    print(\"Best Cross-Validation MSE:\", best_score)\n",
        "\n",
        "    # Evaluate on a test set (if available)\n",
        "    best_model.fit(X_train, y_train)\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    print(f\"MSE of best model: {mean_squared_error(y_test, y_pred)}\")\n",
        "    print(f\"r^2 of best model: {r2_score(y_test, y_pred)}\")\n",
        "\n",
        "    return best_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_state_models(ridge=False, forest=False):\n",
        "    # Load data into X (features) and y (target variable)\n",
        "    target = 'Registration Count'\n",
        "    X = States_ABT.drop(columns=[target, 'EV Registrations per capita'])\n",
        "    y = States_ABT[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    if ridge:\n",
        "        best_ridge_model = make_best_ridge_model(X_train, X_test, y_train, y_test)\n",
        "        best_idx, best_diff_EV_registrations = None, 0\n",
        "        for idx, row in X.iterrows():\n",
        "            hypothetical_row = row.copy()\n",
        "            hypothetical_row['Fuel Station Count'] += 1\n",
        "            y_pred = best_ridge_model.predict([row])\n",
        "            hypothetical_y_pred = best_ridge_model.predict([hypothetical_row])\n",
        "\n",
        "            diff_EV_registrations = float((hypothetical_y_pred - y_pred))\n",
        "            if diff_EV_registrations > best_diff_EV_registrations:\n",
        "                best_idx = idx\n",
        "                best_diff_EV_registrations = diff_EV_registrations\n",
        "\n",
        "        print(best_idx, best_diff_EV_registrations)\n",
        "\n",
        "    if forest:\n",
        "        best_random_forest_model = make_best_random_forest_model(X_train, X_test, y_train, y_test)\n",
        "        best_idx, best_diff_EV_registrations = None, 0\n",
        "        for idx, row in X.iterrows():\n",
        "            hypothetical_row = row.copy()\n",
        "            hypothetical_row['Fuel Station Count'] += 1\n",
        "            y_pred = best_random_forest_model.predict([row])\n",
        "            hypothetical_y_pred = best_random_forest_model.predict([hypothetical_row])\n",
        "\n",
        "            diff_EV_registrations = float((hypothetical_y_pred - y_pred))\n",
        "            if diff_EV_registrations > best_diff_EV_registrations:\n",
        "                best_idx = idx\n",
        "                best_diff_EV_registrations = diff_EV_registrations\n",
        "        print(best_idx, best_diff_EV_registrations)\n",
        "\n",
        "build_state_models(ridge=True)"
      ],
      "metadata": {
        "id": "is0VnlE7CyLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_county_models(ridge=False, forest=False):\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "    # Load data into X (features) and y (target variable)\n",
        "    target = 'EV Registrations'\n",
        "    X = County_ABT.drop(columns=[target, 'EV Registrations per capita'])\n",
        "    y = County_ABT[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    if ridge:\n",
        "        best_ridge_model = make_best_ridge_model(X_train, X_test, y_train, y_test)\n",
        "        best_idx, best_diff_EV_registrations = None, 0\n",
        "        for idx, row in X.iterrows():\n",
        "            hypothetical_row = row.copy()\n",
        "            hypothetical_row['Fuel Station Count'] += 1\n",
        "            y_pred = best_ridge_model.predict([row])\n",
        "            hypothetical_y_pred = best_ridge_model.predict([hypothetical_row])\n",
        "\n",
        "            diff_EV_registrations = float((hypothetical_y_pred - y_pred))\n",
        "            if diff_EV_registrations > best_diff_EV_registrations:\n",
        "                best_idx = idx\n",
        "                best_diff_EV_registrations = diff_EV_registrations\n",
        "\n",
        "        print(best_idx, best_diff_EV_registrations)\n",
        "\n",
        "    if forest:\n",
        "        best_random_forest_model = make_best_random_forest_model(X_train, X_test, y_train, y_test)\n",
        "        best_idx, best_diff_EV_registrations = None, 0\n",
        "        for idx, row in X.iterrows():\n",
        "            hypothetical_row = row.copy()\n",
        "            hypothetical_row['Fuel Station Count'] += 1\n",
        "            y_pred = best_random_forest_model.predict([row])\n",
        "            hypothetical_y_pred = best_random_forest_model.predict([hypothetical_row])\n",
        "\n",
        "            diff_EV_registrations = float((hypothetical_y_pred - y_pred))\n",
        "            if diff_EV_registrations > best_diff_EV_registrations:\n",
        "                best_idx = idx\n",
        "                best_diff_EV_registrations = diff_EV_registrations\n",
        "        print(best_idx, best_diff_EV_registrations)"
      ],
      "metadata": {
        "id": "ipd4N51sDlE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "build_county_models(ridge=True, forest=False)"
      ],
      "metadata": {
        "id": "XgOh5zFNLrsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "build_county_models(ridge=False, forest=True)"
      ],
      "metadata": {
        "id": "ALFzjnczLss8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}